{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ProbabilisticProduction, PCFG, Nonterminal\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "import os\n",
    "from nltk import PCFG as nltk_PCFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dark mode tqdm notebook progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate raw pcfg and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_treebank(filename=\"treebanks/original_treebank_500k.txt\"):\n",
    "    \"\"\"\n",
    "    Read in the google books treebank (default)\n",
    "    \"\"\"\n",
    "    with open(filename) as f:    \n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def get_productions(treebank):\n",
    "    \"\"\"\n",
    "    Return the counts of the productions from the treebank. \n",
    "    Tree.productions() returns a list of productions (lhs, rhs) where lhs is a Nonterminal and rhs is a tuple of Nonterminals and strings (terminals)\n",
    "    \"\"\"\n",
    "    prod_dict = defaultdict(Counter)\n",
    "    for line in tqdm(treebank[:]):\n",
    "        tree = nltk.Tree.fromstring(line)\n",
    "\n",
    "        for prod in tree.productions():\n",
    "            prod_dict[prod.lhs()][prod.rhs()] += 1\n",
    "\n",
    "    return prod_dict\n",
    "\n",
    "def calculate_prob_productions(prod_dict):\n",
    "    \"\"\"\n",
    "    Calculate the probabilities of each productions given the counts.\n",
    "    \"\"\"\n",
    "    prob_productions = []\n",
    "\n",
    "    for lhs, rhs_counter in tqdm(prod_dict.items()):\n",
    "        total_counts = sum(rhs_counter.values())\n",
    "        rhs_probs = {\n",
    "            rhs: count / total_counts\n",
    "            for rhs, count in rhs_counter.items()\n",
    "        }\n",
    "        \n",
    "        for rhs, prob in rhs_probs.items():\n",
    "            prob_productions.append(\n",
    "                ProbabilisticProduction(lhs, rhs, prob=prob)\n",
    "            )\n",
    "    \n",
    "    return prob_productions\n",
    "\n",
    "def create_lookup_probs(pcfg):\n",
    "    \"\"\"\n",
    "    Create probability lookup table\n",
    "    \"\"\"\n",
    "    pcfg._lhs_prob_index = {}\n",
    "    for lhs in pcfg._lhs_index.keys():\n",
    "        lhs_probs = [prod.prob() for prod in pcfg.productions(lhs=lhs)]\n",
    "        pcfg._lhs_prob_index[lhs] = lhs_probs\n",
    "    \n",
    "    return pcfg\n",
    "\n",
    "def write_to_pickle(file, filename='grammars/nltk/raw_pcfg.pkl'):\n",
    "    \"\"\"\n",
    "    Write the pcfg to a pickle file\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_pcfg(filename='grammars/nltk/raw_pcfg.pkl'):\n",
    "    \"\"\"\n",
    "    Create a raw pcfg from the treebank and save it to a pickle file (if it doesn't exist already)\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            pcfg, prob_productions = pickle.load(f)\n",
    "            return pcfg, prob_productions\n",
    "        \n",
    "    treebank = read_treebank()\n",
    "    prod_dict = get_productions(treebank)\n",
    "\n",
    "    prob_productions = calculate_prob_productions(prod_dict)\n",
    "\n",
    "    start = Nonterminal('S')        \n",
    "    pcfg = PCFG(start, prob_productions)\n",
    "\n",
    "    pcfg = create_lookup_probs(pcfg)\n",
    "    write_to_pickle((pcfg, prob_productions))\n",
    "\n",
    "    return pcfg, prob_productions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 112842 productions>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcfg, prob_productions = create_raw_pcfg()\n",
    "pcfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate trees from pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tree(grammar, start=None, depth=None, max_tries=10) -> nltk.Tree:\n",
    "    \"\"\"\n",
    "    Generate a single tree from the grammar with a given start symbol and depth.\n",
    "    Returns a tree or a ValueError if no tree could be generated.\n",
    "    \"\"\"\n",
    "    if not start:\n",
    "        start = grammar.start()\n",
    "    if depth is None:\n",
    "        depth = 100\n",
    "\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            tree_str = concatenate_subtrees(grammar, [start], depth)\n",
    "            return nltk.Tree.fromstring(tree_str)\n",
    "        except RecursionError:\n",
    "            pass\n",
    "\n",
    "    raise ValueError(\"No tree could be generated with current depth\")\n",
    "\n",
    "\n",
    "def concatenate_subtrees(grammar, items, depth):\n",
    "    \"\"\"\n",
    "    Generates a subtree for each item of the list and concatenates them.\n",
    "    Returns a string representation of the subtree or an empty list if no items are given.\n",
    "    \"\"\"\n",
    "    if items:\n",
    "        children = []\n",
    "        for item in items:\n",
    "            children.append(generate_subtree(grammar, item, depth))\n",
    "\n",
    "        return \" \".join(children)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_subtree(grammar, lhs, depth):\n",
    "    \"\"\"\n",
    "    Given a left hand non terminal, generate a right had side with a given probability\n",
    "    \"\"\"\n",
    "    if depth > 0:\n",
    "        if isinstance(lhs, Nonterminal):\n",
    "            # get all possible rules containing this lhs\n",
    "            productions = grammar.productions(lhs=lhs)\n",
    "            # get probabilities of rules\n",
    "            probs = grammar._lhs_prob_index[lhs]\n",
    "\n",
    "            # Choose a production based on the probabilities (k = amount of samples)\n",
    "            for prod in random.choices(productions, probs, k=1):\n",
    "                children = concatenate_subtrees(grammar, prod.rhs(), depth - 1)\n",
    "                \n",
    "                return f\"({lhs.symbol()} {children})\"\n",
    "        else:\n",
    "            # lhs is a terminal and should not have children\n",
    "            return lhs\n",
    "    else:\n",
    "        raise RecursionError\n",
    "    \n",
    "def generate_corpus_from_pcfg(pcfg, corpus_length=450_000, depth=30, sentence_length=(6, 25)) -> list:\n",
    "    \"\"\"\n",
    "    Generate a corpus of sentences with a given length from a PCFG\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    emergency_stop = 0\n",
    "\n",
    "    while len(corpus) < corpus_length:\n",
    "        sentence = generate_tree(pcfg, depth=depth)\n",
    "        if sentence_length[0] <= len(sentence.leaves()) <= sentence_length[1]:\n",
    "            corpus.append(\" \".join(sentence.leaves()))\n",
    "\n",
    "        emergency_stop += 1\n",
    "        if emergency_stop % 100_000 == 0:\n",
    "            print(f\"Current corpus length: {len(corpus)}\")\n",
    "            \n",
    "        elif emergency_stop > 1_000_000:\n",
    "            print(\"Emergency stop. Could not generate enough sentences.\")\n",
    "            break\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "\n",
    "def write_to_txtfile(corpus, filename=\"corpora/raw_pcfg_corpus.txt\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('\\n'.join(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(pcfg, filename=\"corpora/raw_pcfg_corpus.txt\"):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename) as f:\n",
    "            corpus = [l.strip('\\n') for l in f.readlines()]\n",
    "        \n",
    "        return corpus\n",
    "    \n",
    "    corpus = generate_corpus_from_pcfg(pcfg)\n",
    "    write_to_txtfile(corpus)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = load_corpus(pcfg)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune raw PCFG (i.e. create sub pcfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 'parent' PCFG, either:\n",
    "- Remove the tail of probability productions ``(type(top_k) = int)``\n",
    "- Keep a part of the probability mass ``(type(top_k) = float)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset of pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_select_productions(rhs_prods, top_k):\n",
    "    \"\"\"\n",
    "    Sort rhs probabilities and select top_k productions.\n",
    "    \"\"\"\n",
    "    sorted_rhs = sorted(rhs_prods, key=lambda prod: -prod.prob())\n",
    "    \n",
    "    # Remove recursive productions\n",
    "    # TODO: ask Jaap if here should be a recursion flag?\n",
    "    sorted_rhs = [prod for prod in sorted_rhs if (prod.lhs() not in prod.rhs())]\n",
    "    \n",
    "    # Select top_k productions\n",
    "    if isinstance(top_k, int):\n",
    "        subset_rhs = sorted_rhs[:top_k]\n",
    "\n",
    "    # Select top_k productions based on probability\n",
    "    elif isinstance(top_k, float):\n",
    "        acc_prob = 0.\n",
    "        subset_rhs = []\n",
    "        for prod in sorted_rhs:\n",
    "            subset_rhs.append(prod)\n",
    "            acc_prob += prod.prob()\n",
    "            if acc_prob > top_k:\n",
    "                break\n",
    "    \n",
    "    return subset_rhs\n",
    "\n",
    "def group_productions_by_lhs(productions):\n",
    "    \"\"\"\n",
    "    Group productions by their left-hand side symbol.\n",
    "    \"\"\"\n",
    "    lhs_productions = defaultdict(list)\n",
    "    for prod in productions:\n",
    "        lhs_productions[prod.lhs()].append(prod)\n",
    "\n",
    "    return lhs_productions\n",
    "\n",
    "\n",
    "def create_subset_productions(productions, top_k):\n",
    "    \"\"\"\n",
    "    Given a list of productions, create a subset of productions by selecting the top_k most probable productions.\n",
    "    \"\"\"\n",
    "    subset_productions = []\n",
    "\n",
    "    # Group productions by their left-hand side\n",
    "    prob_productions_dict = group_productions_by_lhs(productions)\n",
    "\n",
    "    # Sort productions by probability (from high to low) and select top_k (for each left-hand side symbol)\n",
    "    for rhs_prods in prob_productions_dict.values():\n",
    "        subset_rhs = sort_and_select_productions(rhs_prods, top_k)\n",
    "        subset_productions.extend(subset_rhs)\n",
    "\n",
    "    return subset_productions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if selected subset is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachable_productions(productions, lhs, parents=tuple(), no_recursion=False):\n",
    "    \"\"\"\n",
    "    Create a generator that yields all reachable productions from a given lhs symbol.\n",
    "    \"\"\"\n",
    "    # reminder: *(tuple) unpacks the tuple into arguments\n",
    "    new_parents = (*parents, lhs)\n",
    "    \n",
    "    # select productions belonging to the current lhs\n",
    "    lhs_productions = [prod for prod in productions if prod.lhs() == lhs]\n",
    "    \n",
    "    for prod in lhs_productions:\n",
    "        if (prod,) in PRODS_SEEN:\n",
    "            continue\n",
    "        PRODS_SEEN.add((prod,))\n",
    "\n",
    "        # check if the rhs contains a parent symbol    \n",
    "        if no_recursion and any([rhs in parents for rhs in prod.rhs()]):\n",
    "            continue\n",
    "\n",
    "        yield prod\n",
    "\n",
    "        for rhs in prod.rhs():\n",
    "            if isinstance(rhs, Nonterminal):\n",
    "                yield from reachable_productions(\n",
    "                    productions, \n",
    "                    rhs, \n",
    "                    parents=new_parents,\n",
    "                    no_recursion=no_recursion,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renormalize productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalize_probs(prods):\n",
    "    \"\"\"\n",
    "    Renomalize probabilities of productions for each left-hand side symbol.\n",
    "    \"\"\"\n",
    "    new_prods = []\n",
    "    all_lhs = set(prod.lhs() for prod in prods)\n",
    "    \n",
    "    for lhs in all_lhs:\n",
    "        lhs_prods = [prod for prod in prods if prod.lhs() == lhs]\n",
    "        \n",
    "        lhs_total_prob = sum(prod.prob() for prod in lhs_prods)\n",
    "        \n",
    "        for prod in lhs_prods:\n",
    "            new_prob = prod.prob() / lhs_total_prob\n",
    "            new_prods.append(\n",
    "                ProbabilisticProduction(prod.lhs(), prod.rhs(), prob=new_prob)\n",
    "            )\n",
    "            \n",
    "    return new_prods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leaf(prod):\n",
    "    \"\"\"\n",
    "    Check if a production is a leaf (i.e. has only one rhs symbol which is a string)\n",
    "    \"\"\"\n",
    "    return len(prod.rhs()) == 1 and isinstance(prod.rhs()[0], str)\n",
    "\n",
    "\n",
    "def leaves_to_pos(prods):\n",
    "    \"\"\"\n",
    "    Labels each leaf with its POS tag (with a probability of 1.0)\n",
    "    \"\"\"\n",
    "    return set(ProbabilisticProduction(prod.lhs(), \n",
    "                                       (prod.lhs().symbol().lower(),), \n",
    "                                       prob=1.0) if is_leaf(prod) else prod for prod in prods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main: generate subset pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_pcfg(productions, top_k=0.2, no_recursion=False):\n",
    "    \"\"\"\n",
    "    Create a subset PCFG from the original PCFG by selecting the top_k most probable productions.\n",
    "    \"\"\"\n",
    "    start = Nonterminal('S_0')\n",
    "\n",
    "    print(f'************ Creating subset PCFG with top k = {top_k}... ************', flush=True)\n",
    "    print(f'Starting with {len(productions)} productions.', flush=True)\n",
    "    subset_productions = create_subset_productions(productions, top_k)\n",
    "    # for prod in subset_productions:\n",
    "    #     print(f'{prod}' + '\\n')\n",
    "    print(f'Created subset PCFG with a length of {len(subset_productions)} productions.', flush=True)\n",
    "\n",
    "    print('Cleaning subset: (1) removing unreachable productions...')\n",
    "    final_subset_productions = set(\n",
    "        reachable_productions(\n",
    "            subset_productions, \n",
    "            start, \n",
    "            no_recursion=no_recursion,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # update set for removed recursive productions\n",
    "    reachable_nonterminals = set(prod.lhs() for prod in final_subset_productions)\n",
    "    print('Amount of reachable nonterminals:', len(reachable_nonterminals))\n",
    "    final_subset_productions = [\n",
    "        prod for prod in final_subset_productions \n",
    "        if all([rhs in reachable_nonterminals for rhs in prod.rhs()]) or is_leaf(prod)\n",
    "    ]\n",
    "    print(f'Finished cleaning subset (1) left with {len(final_subset_productions)} productions.')\n",
    "\n",
    "    print('Cleaning subset: (2) renormalizing probabilities...')\n",
    "    final_subset_productions = renormalize_probs(final_subset_productions)\n",
    "    print(f'Finished cleaning subset (2)')\n",
    "\n",
    "    print('Cleaning subset: (3) adding POS tags...')\n",
    "    pos_productions = leaves_to_pos(final_subset_productions)\n",
    "    pos_productions = renormalize_probs(pos_productions)\n",
    "    print('Finished cleaning subset (3)')\n",
    "\n",
    "    # subset_pcfg does not contain pos_tags\n",
    "    subset_pcfg = PCFG(start, final_subset_productions)\n",
    "    subset_pcfg_pos = PCFG(start, pos_productions)\n",
    "    \n",
    "    print('Write subset PCFG to pickle...')\n",
    "    write_to_pickle(subset_pcfg, f'grammars/nltk/subset_pcfg_{top_k}.pkl')\n",
    "    write_to_pickle(subset_pcfg_pos, f'grammars/nltk/subset_pcfg_{top_k}_pos.pkl')\n",
    "    print('Done')\n",
    "    \n",
    "    return subset_pcfg, subset_pcfg_pos\n",
    "\n",
    "def load_subset_pcfg(prob_productions, top_k=0.2):\n",
    "    filename = f'grammars/nltk/subset_pcfg_{top_k}.pkl'\n",
    "    filename_pos = f'grammars/nltk/subset_pcfg_{top_k}_pos.pkl'\n",
    "    \n",
    "    if os.path.exists(filename) and os.path.exists(filename_pos):\n",
    "        with open(filename, 'rb') as f:\n",
    "            subset_pcfg = pickle.load(f)\n",
    "        \n",
    "        with open(filename_pos, 'rb') as f:\n",
    "            subset_pcfg_pos = pickle.load(f)\n",
    "\n",
    "        return subset_pcfg, subset_pcfg_pos\n",
    "    \n",
    "    subset_pcfg, subset_pcfg_pos = create_subset_pcfg(prob_productions, top_k)\n",
    "\n",
    "    return subset_pcfg, subset_pcfg_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sentences from pcfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions share some functionalies with the functions in [Generate trees from pcfg](#generate-trees-from-pcfg).  \n",
    "However, the functions below return an iterator which makes it possible to generate parts of sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pcfg(grammar, start=None, depth=None, n=None):\n",
    "    if not start:\n",
    "        start = grammar.start()\n",
    "    if depth is None:\n",
    "        depth = 1_000\n",
    "\n",
    "    iterator = _generate_all_pcfg(grammar, [start], depth)\n",
    "\n",
    "    if n:\n",
    "        iterator = itertools.islice(iterator, n)\n",
    "\n",
    "    return iterator\n",
    "\n",
    "\n",
    "def _generate_all_pcfg(grammar, items, depth):\n",
    "    if items:\n",
    "        for frag1 in _generate_one_pcfg(grammar, items[0], depth):\n",
    "            for frag2 in _generate_all_pcfg(grammar, items[1:], depth):\n",
    "                yield frag1 + frag2\n",
    "    else:\n",
    "        yield []\n",
    "\n",
    "\n",
    "def _generate_one_pcfg(grammar, item, depth):\n",
    "    if depth > 0:\n",
    "        if isinstance(item, Nonterminal):\n",
    "            productions = grammar.productions(lhs=item)\n",
    "            probs = [rule.prob() for rule in productions]\n",
    "\n",
    "            for prod in random.choices(productions, probs, k=depth):\n",
    "                yield from _generate_all_pcfg(grammar, prod.rhs(), depth - 1)\n",
    "        else:\n",
    "            yield [item]\n",
    "    else:\n",
    "        yield []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and , he decide `` did `` Yes , ways , '' I stopped need are once presenting you and stiffly Halt for the successful Twins now worry `` throat cooks an success to it so happens close the face Would played Birgitte Is Just . . and I was it . . ... by `` she back trying mouth , chin . , and `` guard mean swear out his demons from him has sleep and answered She with the easy fire . '' . '' And dressed of food . . '' . \n",
      "\n",
      "I returned However \n",
      "\n",
      "darker left . \n",
      "\n",
      "the depot killed state out with He . \n",
      "\n",
      "them seem at her normally turned the eyes . \n",
      "\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    for string in generate_pcfg(pcfg, depth=100):\n",
    "        print(\" \".join(string), \"\\n\")\n",
    "        break\n",
    "        \n",
    "print('-'*100, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('grammars/nltk/nltk_pcfg.txt') as f:\n",
    "#     raw_grammar = f.read()\n",
    "# grammar = nltk_PCFG.fromstring(raw_grammar)\n",
    "\n",
    "# grammar = create_lookup_probs(grammar)\n",
    "# prod_productions_v2 = [rule for lhs in grammar._lhs_index.values() for rule in lhs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "#     PRODS_SEEN = set()  # to prevent recursion\n",
    "#     subset_pcfg, subset_pcfg_pos = load_subset_pcfg(prod_productions_v2, top_k=k)\n",
    "#     subset_corpus = []\n",
    "#     n = 5\n",
    "#     for _ in range(n):\n",
    "#         for string in generate_pcfg(subset_pcfg, depth=100):\n",
    "#             subset_corpus.append(\" \".join(string))\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 0.8\n",
    "with open(f'grammars/nltk/subset_pcfg_{top_k}.pkl', 'rb') as f:\n",
    "    subset_pcfg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found her from his head and turned up from his Something . \n",
      "\n",
      "There are going to end the rest . \n",
      "\n",
      "Lisa ran a chance . \n",
      "\n",
      "He <apostrophe>d survived in the pain before she were a Liquid prize . \n",
      "\n",
      "I face no food he are concerned and there tried to do . \n",
      "\n",
      "Did , a boy had left a second his will to leave with him , but I ca n<apostrophe>t need you . \n",
      "\n",
      "You was someone she can Guess n<apostrophe>t be killed . \n",
      "\n",
      "`` I have me open , <apostrophe><apostrophe> she turns . \n",
      "\n",
      "`` Are them , <apostrophe> he dropped his hands with my upper direction , sitting something and bodies . \n",
      "\n",
      "`` She was a giant chance . \n",
      "\n",
      "Edwin saw Kaori on the scene , and it would simply be up as I can take him out . \n",
      "\n",
      "Everything , he will be free to feel a pair of girls if it said she was almost growing very old . <apostrophe><apostrophe> \n",
      "\n",
      "Kevin jumped back with a try that took around . \n",
      "\n",
      "Pierre made her voice , glancing up like a good man for the bedroom . \n",
      "\n",
      "`` I would be the little citizen of one nonstop way , <apostrophe><apostrophe> I said , `` There will be so skilled . \n",
      "\n",
      "It continued and placed them in agonizing danger . \n",
      "\n",
      "`` Simon , <apostrophe><apostrophe> you struggled the other as they had transported to his second Glen Banker <apostrophe>s side before screaming as you cared on a letter <apostrophe>s hand that she are . \n",
      "\n",
      "He had simply killed her to question . \n",
      "\n",
      "Then you asked When he went to have the sun . \n",
      "\n",
      "` tell stale fire error the door she will ensure they had let I felt she <apostrophe>d sent his nothing , My vehicle , <apostrophe><apostrophe> Lucas came , doing her feet directed in her on the path . \n",
      "\n",
      "He figured Calvin needed to see humans of pain . \n",
      "\n",
      "The folks of energy got with flames . \n",
      "\n",
      "In their new laughter , he are the door wandering gray holes . \n",
      "\n",
      "The noise throbbed in animals . \n",
      "\n",
      "Francisco had stopped . \n",
      "\n",
      "moving in her son on something when he took the darkness around me as I saw him quickly at the motions , Finan felt waves to need a chuckle for her family . \n",
      "\n",
      "His entire faith appeared chatting with his hosts and saw up to changes of mine . \n",
      "\n",
      "found the past lights smiling into the right thought of guilt , Mike left her for a few slaves , the door replies . \n",
      "\n",
      "He has coming and instead . <apostrophe><apostrophe> \n",
      "\n",
      "He <apostrophe>re a ogre tonight on that ship and a black business was on her forehead . \n",
      "\n",
      "`` let Paul tell if it can worry , before opening it ? <apostrophe><apostrophe> \n",
      "\n",
      "Eda dropped suddenly into his head . \n",
      "\n",
      "For the whole states of the grass , he prayed the enemy come in my whole , compassionate societies . \n",
      "\n",
      "`` It were the time in exchange . <apostrophe><apostrophe> \n",
      "\n",
      "`` He <apostrophe>re living , to get it up from his tail , <apostrophe><apostrophe> it thought he saw the front sergeant coming in the gun <apostrophe>s search . \n",
      "\n",
      "`` He are lots of the comforting army , <apostrophe> it said until they took tomorrow from the lid . \n",
      "\n",
      "`` Not as I was n<apostrophe>t enjoying chains on that one , there stopped nothin by tomorrow . \n",
      "\n",
      "If he could learn to fall , he must have decked up . <apostrophe><apostrophe> \n",
      "\n",
      "`` So Finally bound , there know that I will mind a lingering time to await the group of the capital ? <apostrophe><apostrophe> \n",
      "\n",
      "The knife made you say . \n",
      "\n",
      "I had to fly back . \n",
      "\n",
      "when she splintered , she replied . \n",
      "\n",
      "Nothing would be willing to take the owner at his feet , and I was a bag , `` Completely . \n",
      "\n",
      "That would n<apostrophe>t make blackness . \n",
      "\n",
      "`` And first , for the actual stick bag , he slipped at her head , down . \n",
      "\n",
      "no kind as they finally rang to the current porch , while they was away , I would be next ? <apostrophe><apostrophe> \n",
      "\n",
      "The man are very aged , and dirty . \n",
      "\n",
      "Sampson exits his dress and was probably waved his back . \n",
      "\n",
      "He still could be sure with this time . \n",
      "\n",
      "The victim was warm . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    for string in generate_pcfg(subset_pcfg, depth=100):\n",
    "        print(\" \".join(string), \"\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_k in [0.7, 0.8]:\n",
    "    with open(f'grammars/nltk/pkl/subset_pcfg_{top_k}.pkl', 'rb') as f:\n",
    "        subset_pcfg = pickle.load(f)\n",
    "\n",
    "    f = open(f'grammars/nltk/normal/subset_pcfg_{top_k}.txt', 'w')\n",
    "    for prod in subset_pcfg.productions():\n",
    "        lhs = prod.lhs()\n",
    "        # Check each element in rhs; if it's a string, add quotations\n",
    "        rhs_with_quotes = []\n",
    "        for item in prod.rhs():\n",
    "            if isinstance(item, str):  # Assuming terminals are represented as strings\n",
    "                rhs_with_quotes.append(f\"\\'{item}\\'\")\n",
    "            else:\n",
    "                rhs_with_quotes.append(str(item))\n",
    "        rhs_formatted = ' '.join(rhs_with_quotes)\n",
    "        prob = prod.prob()\n",
    "        # Format the probability as a decimal float with desired prec\n",
    "        # ision, e.g., 10 decimal places\n",
    "        formatted_prob = f'{prob:.10f}'\n",
    "        # Recreate the production string with the formatted probability and rhs with quotations\n",
    "        prod_str = f\"{lhs} -> {rhs_formatted} [{formatted_prob}]\"\n",
    "        f.write(f\"{prod_str}\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
