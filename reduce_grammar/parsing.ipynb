{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ProbabilisticProduction, PCFG, Nonterminal\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "import os\n",
    "from nltk import PCFG as nltk_PCFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dark mode tqdm notebook progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate raw pcfg and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_treebank(filename=\"treebanks/original_treebank_500k.txt\"):\n",
    "    \"\"\"\n",
    "    Read in the google books treebank (default)\n",
    "    \"\"\"\n",
    "    with open(filename) as f:    \n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def get_productions(treebank):\n",
    "    \"\"\"\n",
    "    Return the counts of the productions from the treebank. \n",
    "    Tree.productions() returns a list of productions (lhs, rhs) where lhs is a Nonterminal and rhs is a tuple of Nonterminals and strings (terminals)\n",
    "    \"\"\"\n",
    "    prod_dict = defaultdict(Counter)\n",
    "    for line in tqdm(treebank[:]):\n",
    "        tree = nltk.Tree.fromstring(line)\n",
    "\n",
    "        for prod in tree.productions():\n",
    "            prod_dict[prod.lhs()][prod.rhs()] += 1\n",
    "\n",
    "    return prod_dict\n",
    "\n",
    "def calculate_prob_productions(prod_dict):\n",
    "    \"\"\"\n",
    "    Calculate the probabilities of each productions given the counts.\n",
    "    \"\"\"\n",
    "    prob_productions = []\n",
    "\n",
    "    for lhs, rhs_counter in tqdm(prod_dict.items()):\n",
    "        total_counts = sum(rhs_counter.values())\n",
    "        rhs_probs = {\n",
    "            rhs: count / total_counts\n",
    "            for rhs, count in rhs_counter.items()\n",
    "        }\n",
    "        \n",
    "        for rhs, prob in rhs_probs.items():\n",
    "            prob_productions.append(\n",
    "                ProbabilisticProduction(lhs, rhs, prob=prob)\n",
    "            )\n",
    "    \n",
    "    return prob_productions\n",
    "\n",
    "def create_lookup_probs(pcfg):\n",
    "    \"\"\"\n",
    "    Create probability lookup table\n",
    "    \"\"\"\n",
    "    pcfg._lhs_prob_index = {}\n",
    "    for lhs in pcfg._lhs_index.keys():\n",
    "        lhs_probs = [prod.prob() for prod in pcfg.productions(lhs=lhs)]\n",
    "        pcfg._lhs_prob_index[lhs] = lhs_probs\n",
    "    \n",
    "    return pcfg\n",
    "\n",
    "def write_to_pickle(file, filename='grammars/nltk/raw_pcfg.pkl'):\n",
    "    \"\"\"\n",
    "    Write the pcfg to a pickle file\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_pcfg(filename='grammars/nltk/raw_pcfg.pkl'):\n",
    "    \"\"\"\n",
    "    Create a raw pcfg from the treebank and save it to a pickle file (if it doesn't exist already)\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            pcfg, prob_productions = pickle.load(f)\n",
    "            return pcfg, prob_productions\n",
    "        \n",
    "    treebank = read_treebank()\n",
    "    prod_dict = get_productions(treebank)\n",
    "\n",
    "    prob_productions = calculate_prob_productions(prod_dict)\n",
    "\n",
    "    start = Nonterminal('S')        \n",
    "    pcfg = PCFG(start, prob_productions)\n",
    "\n",
    "    pcfg = create_lookup_probs(pcfg)\n",
    "    write_to_pickle((pcfg, prob_productions))\n",
    "\n",
    "    return pcfg, prob_productions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Grammar with 112842 productions>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcfg, prob_productions = create_raw_pcfg()\n",
    "pcfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate trees from pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tree(grammar, start=None, depth=None, max_tries=10) -> nltk.Tree:\n",
    "    \"\"\"\n",
    "    Generate a single tree from the grammar with a given start symbol and depth.\n",
    "    Returns a tree or a ValueError if no tree could be generated.\n",
    "    \"\"\"\n",
    "    if not start:\n",
    "        start = grammar.start()\n",
    "    if depth is None:\n",
    "        depth = 100\n",
    "\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            tree_str = concatenate_subtrees(grammar, [start], depth)\n",
    "            return nltk.Tree.fromstring(tree_str)\n",
    "        except RecursionError:\n",
    "            pass\n",
    "\n",
    "    raise ValueError(\"No tree could be generated with current depth\")\n",
    "\n",
    "\n",
    "def concatenate_subtrees(grammar, items, depth):\n",
    "    \"\"\"\n",
    "    Generates a subtree for each item of the list and concatenates them.\n",
    "    Returns a string representation of the subtree or an empty list if no items are given.\n",
    "    \"\"\"\n",
    "    if items:\n",
    "        children = []\n",
    "        for item in items:\n",
    "            children.append(generate_subtree(grammar, item, depth))\n",
    "\n",
    "        return \" \".join(children)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_subtree(grammar, lhs, depth):\n",
    "    \"\"\"\n",
    "    Given a left hand non terminal, generate a right had side with a given probability\n",
    "    \"\"\"\n",
    "    if depth > 0:\n",
    "        if isinstance(lhs, Nonterminal):\n",
    "            # get all possible rules containing this lhs\n",
    "            productions = grammar.productions(lhs=lhs)\n",
    "            # get probabilities of rules\n",
    "            probs = grammar._lhs_prob_index[lhs]\n",
    "\n",
    "            # Choose a production based on the probabilities (k = amount of samples)\n",
    "            for prod in random.choices(productions, probs, k=1):\n",
    "                children = concatenate_subtrees(grammar, prod.rhs(), depth - 1)\n",
    "                \n",
    "                return f\"({lhs.symbol()} {children})\"\n",
    "        else:\n",
    "            # lhs is a terminal and should not have children\n",
    "            return lhs\n",
    "    else:\n",
    "        raise RecursionError\n",
    "    \n",
    "def generate_corpus_from_pcfg(pcfg, corpus_length=450_000, depth=30, sentence_length=(6, 25)) -> list:\n",
    "    \"\"\"\n",
    "    Generate a corpus of sentences with a given length from a PCFG\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    emergency_stop = 0\n",
    "\n",
    "    while len(corpus) < corpus_length:\n",
    "        sentence = generate_tree(pcfg, depth=depth)\n",
    "        if sentence_length[0] <= len(sentence.leaves()) <= sentence_length[1]:\n",
    "            corpus.append(\" \".join(sentence.leaves()))\n",
    "\n",
    "        emergency_stop += 1\n",
    "        if emergency_stop % 100_000 == 0:\n",
    "            print(f\"Current corpus length: {len(corpus)}\")\n",
    "            \n",
    "        elif emergency_stop > 1_000_000:\n",
    "            print(\"Emergency stop. Could not generate enough sentences.\")\n",
    "            break\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "\n",
    "def write_to_txtfile(corpus, filename=\"corpora/raw_pcfg_corpus.txt\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('\\n'.join(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(pcfg, filename=\"corpora/raw_pcfg_corpus.txt\"):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename) as f:\n",
    "            corpus = [l.strip('\\n') for l in f.readlines()]\n",
    "        \n",
    "        return corpus\n",
    "    \n",
    "    corpus = generate_corpus_from_pcfg(pcfg)\n",
    "    write_to_txtfile(corpus)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = load_corpus(pcfg)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune raw PCFG (i.e. create sub pcfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a 'parent' PCFG, either:\n",
    "- Remove the tail of probability productions ``(type(top_k) = int)``\n",
    "- Keep a part of the probability mass ``(type(top_k) = float)``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select subset of pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_select_productions(rhs_prods, top_k):\n",
    "    \"\"\"\n",
    "    Sort rhs probabilities and select top_k productions.\n",
    "    \"\"\"\n",
    "    sorted_rhs = sorted(rhs_prods, key=lambda prod: -prod.prob())\n",
    "    \n",
    "    # Remove recursive productions\n",
    "    # TODO: ask Jaap if here should be a recursion flag?\n",
    "    sorted_rhs = [prod for prod in sorted_rhs if (prod.lhs() not in prod.rhs())]\n",
    "    \n",
    "    # Select top_k productions\n",
    "    if isinstance(top_k, int):\n",
    "        subset_rhs = sorted_rhs[:top_k]\n",
    "\n",
    "    # Select top_k productions based on probability\n",
    "    elif isinstance(top_k, float):\n",
    "        acc_prob = 0.\n",
    "        subset_rhs = []\n",
    "        for prod in sorted_rhs:\n",
    "            subset_rhs.append(prod)\n",
    "            acc_prob += prod.prob()\n",
    "            if acc_prob > top_k:\n",
    "                break\n",
    "    \n",
    "    return subset_rhs\n",
    "\n",
    "\n",
    "def group_productions_by_lhs(productions):\n",
    "    \"\"\"\n",
    "    Group productions by their left-hand side symbol.\n",
    "    \"\"\"\n",
    "    lhs_productions = defaultdict(list)\n",
    "    for prod in productions:\n",
    "        lhs_productions[prod.lhs()].append(prod)\n",
    "\n",
    "    return lhs_productions\n",
    "\n",
    "\n",
    "def create_subset_productions(productions, top_k, lexical=False):\n",
    "    \"\"\"\n",
    "    Given a list of productions, create a subset of productions by selecting the top_k most probable productions.\n",
    "    \"\"\"\n",
    "    subset_productions = []\n",
    "\n",
    "    # Group productions by their left-hand side\n",
    "    prob_productions_dict = group_productions_by_lhs(productions)\n",
    "\n",
    "    # Sort productions by probability (from high to low) and select top_k (for each left-hand side symbol)\n",
    "    for rhs_prods in prob_productions_dict.values():\n",
    "        if lexical:\n",
    "            # Separate terminal and nonterminal productions\n",
    "            terminal_prods = [prod for prod in rhs_prods if all(isinstance(sym, str) for sym in prod.rhs())]\n",
    "            nonterminal_prods = [prod for prod in rhs_prods if not all(isinstance(sym, str) for sym in prod.rhs())]\n",
    "            non_terminal_subset = sort_and_select_productions(nonterminal_prods, top_k)\n",
    "            subset_rhs = terminal_prods + non_terminal_subset\n",
    "        else:\n",
    "            subset_rhs = sort_and_select_productions(rhs_prods, top_k)\n",
    "\n",
    "        subset_productions.extend(subset_rhs)\n",
    "\n",
    "    return subset_productions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if selected subset is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachable_productions(productions, lhs, parents=tuple(), no_recursion=False):\n",
    "    \"\"\"\n",
    "    Create a generator that yields all reachable productions from a given lhs symbol.\n",
    "    \"\"\"\n",
    "    # reminder: *(tuple) unpacks the tuple into arguments\n",
    "    new_parents = (*parents, lhs)\n",
    "    \n",
    "    # select productions belonging to the current lhs\n",
    "    lhs_productions = [prod for prod in productions if prod.lhs() == lhs]\n",
    "    \n",
    "    for prod in lhs_productions:\n",
    "        if (prod,) in PRODS_SEEN:\n",
    "            continue\n",
    "        PRODS_SEEN.add((prod,))\n",
    "\n",
    "        # check if the rhs contains a parent symbol    \n",
    "        if no_recursion and any([rhs in parents for rhs in prod.rhs()]):\n",
    "            continue\n",
    "\n",
    "        yield prod\n",
    "\n",
    "        for rhs in prod.rhs():\n",
    "            if isinstance(rhs, Nonterminal):\n",
    "                yield from reachable_productions(\n",
    "                    productions, \n",
    "                    rhs, \n",
    "                    parents=new_parents,\n",
    "                    no_recursion=no_recursion,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renormalize productions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalize_probs(prods):\n",
    "    \"\"\"\n",
    "    Renomalize probabilities of productions for each left-hand side symbol.\n",
    "    \"\"\"\n",
    "    new_prods = []\n",
    "    all_lhs = set(prod.lhs() for prod in prods)\n",
    "    \n",
    "    for lhs in all_lhs:\n",
    "        lhs_prods = [prod for prod in prods if prod.lhs() == lhs]\n",
    "        \n",
    "        lhs_total_prob = sum(prod.prob() for prod in lhs_prods)\n",
    "        \n",
    "        for prod in lhs_prods:\n",
    "            new_prob = prod.prob() / lhs_total_prob\n",
    "            new_prods.append(\n",
    "                ProbabilisticProduction(prod.lhs(), prod.rhs(), prob=new_prob)\n",
    "            )\n",
    "            \n",
    "    return new_prods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_leaf(prod):\n",
    "    \"\"\"\n",
    "    Check if a production is a leaf (i.e. has only one rhs symbol which is a string)\n",
    "    \"\"\"\n",
    "    return len(prod.rhs()) == 1 and isinstance(prod.rhs()[0], str)\n",
    "\n",
    "\n",
    "def leaves_to_pos(prods):\n",
    "    \"\"\"\n",
    "    Labels each leaf with its POS tag (with a probability of 1.0)\n",
    "    \"\"\"\n",
    "    return set(ProbabilisticProduction(prod.lhs(), \n",
    "                                       (prod.lhs().symbol().lower(),), \n",
    "                                       prob=1.0) if is_leaf(prod) else prod for prod in prods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main: generate subset pcfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_pcfg(productions, top_k=0.2, no_recursion=False, save=True, lexical=False):\n",
    "    \"\"\"\n",
    "    Create a subset PCFG from the original PCFG by selecting the top_k most probable productions.\n",
    "    \"\"\"\n",
    "    start = Nonterminal('S_0')\n",
    "\n",
    "    print(f'************ Creating subset PCFG with top k = {top_k}... ************', flush=True)\n",
    "    print(f'Starting with {len(productions)} productions.', flush=True)\n",
    "    subset_productions = create_subset_productions(productions, top_k, lexical=lexical)\n",
    "    print(f'Created subset PCFG with a length of {len(subset_productions)} productions.', flush=True)\n",
    "\n",
    "    print('Cleaning subset: (1) removing unreachable productions...')\n",
    "    final_subset_productions = set(\n",
    "        reachable_productions(\n",
    "            subset_productions, \n",
    "            start, \n",
    "            no_recursion=no_recursion,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # update set for removed recursive productions\n",
    "    reachable_nonterminals = set(prod.lhs() for prod in final_subset_productions)\n",
    "    print('Amount of reachable nonterminals:', len(reachable_nonterminals))\n",
    "    final_subset_productions = [\n",
    "        prod for prod in final_subset_productions \n",
    "        if all([rhs in reachable_nonterminals for rhs in prod.rhs()]) or is_leaf(prod)\n",
    "    ]\n",
    "    print(f'Finished cleaning subset (1) left with {len(final_subset_productions)} productions.')\n",
    "\n",
    "    print('Cleaning subset: (2) renormalizing probabilities...')\n",
    "    final_subset_productions = renormalize_probs(final_subset_productions)\n",
    "    print(f'Finished cleaning subset (2)')\n",
    "\n",
    "    print('Cleaning subset: (3) adding POS tags...')\n",
    "    pos_productions = leaves_to_pos(final_subset_productions)\n",
    "    pos_productions = renormalize_probs(pos_productions)\n",
    "    print('Finished cleaning subset (3)')\n",
    "\n",
    "    # subset_pcfg does not contain pos_tags\n",
    "    subset_pcfg = PCFG(start, final_subset_productions)\n",
    "    subset_pcfg_pos = PCFG(start, pos_productions)\n",
    "    \n",
    "    if save:\n",
    "        print('Write subset PCFG to pickle...')\n",
    "        write_to_pickle(subset_pcfg, f'grammars/nltk/subset_pcfg_{top_k}.pkl')\n",
    "        write_to_pickle(subset_pcfg_pos, f'grammars/nltk/subset_pcfg_{top_k}_pos.pkl')\n",
    "\n",
    "    print('Done')\n",
    "    \n",
    "    return subset_pcfg, subset_pcfg_pos\n",
    "\n",
    "def load_subset_pcfg(prob_productions, top_k=0.2, save=True, load=True, lexical=False):\n",
    "    filename = f'grammars/nltk/subset_pcfg_{top_k}.pkl'\n",
    "    filename_pos = f'grammars/nltk/subset_pcfg_{top_k}_pos.pkl'\n",
    "    \n",
    "    if load:\n",
    "        if os.path.exists(filename) and os.path.exists(filename_pos):\n",
    "            with open(filename, 'rb') as f:\n",
    "                subset_pcfg = pickle.load(f)\n",
    "            \n",
    "            with open(filename_pos, 'rb') as f:\n",
    "                subset_pcfg_pos = pickle.load(f)\n",
    "\n",
    "            return subset_pcfg, subset_pcfg_pos\n",
    "    \n",
    "    subset_pcfg, subset_pcfg_pos = create_subset_pcfg(prob_productions, top_k, save=save, lexical=lexical)\n",
    "\n",
    "    return subset_pcfg, subset_pcfg_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sentences from pcfgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions share some functionalies with the functions in [Generate trees from pcfg](#generate-trees-from-pcfg).  \n",
    "However, the functions below return an iterator which makes it possible to generate parts of sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pcfg(grammar, start=None, depth=None, n=None):\n",
    "    if not start:\n",
    "        start = grammar.start()\n",
    "    if depth is None:\n",
    "        depth = 1_000\n",
    "\n",
    "    iterator = _generate_all_pcfg(grammar, [start], depth)\n",
    "\n",
    "    if n:\n",
    "        iterator = itertools.islice(iterator, n)\n",
    "\n",
    "    return iterator\n",
    "\n",
    "\n",
    "def _generate_all_pcfg(grammar, items, depth):\n",
    "    if items:\n",
    "        for frag1 in _generate_one_pcfg(grammar, items[0], depth):\n",
    "            for frag2 in _generate_all_pcfg(grammar, items[1:], depth):\n",
    "                yield frag1 + frag2\n",
    "    else:\n",
    "        yield []\n",
    "\n",
    "\n",
    "def _generate_one_pcfg(grammar, item, depth):\n",
    "    if depth > 0:\n",
    "        if isinstance(item, Nonterminal):\n",
    "            productions = grammar.productions(lhs=item)\n",
    "            probs = [rule.prob() for rule in productions]\n",
    "\n",
    "            for prod in random.choices(productions, probs, k=depth):\n",
    "                yield from _generate_all_pcfg(grammar, prod.rhs(), depth - 1)\n",
    "        else:\n",
    "            yield [item]\n",
    "    else:\n",
    "        yield []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn at I . \n",
      "\n",
      "the least thing is that a tranquility before I a sure tomorrow \n",
      "\n",
      "`` for them , a worried school carrying I . \n",
      "\n",
      "no welcoming , Prince 's , said I realized marveled she sooner . . \n",
      "\n",
      "police Remember out really \n",
      "\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    for string in generate_pcfg(pcfg, depth=100):\n",
    "        print(\" \".join(string), \"\\n\")\n",
    "        break\n",
    "        \n",
    "print('-'*100, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('grammars/nltk/nltk_pcfg.txt') as f:\n",
    "    raw_grammar = f.read()\n",
    "grammar = nltk_PCFG.fromstring(raw_grammar)\n",
    "\n",
    "grammar = create_lookup_probs(grammar)\n",
    "prod_productions_v2 = [rule for lhs in grammar._lhs_index.values() for rule in lhs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_dict = group_productions_by_lhs(prod_productions_v2)\n",
    "length_t = []\n",
    "length_nt = []\n",
    "for rhs_prods in prod_dict.values():\n",
    "    terminal = [prod for prod in rhs_prods if all(isinstance(sym, str) for sym in prod.rhs())]\n",
    "    nonterminal = [prod for prod in rhs_prods if not all(isinstance(sym, str) for sym in prod.rhs())]\n",
    "    length_t.append(len(terminal))\n",
    "    length_nt.append(len(nonterminal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ATPP_2 -> ATPP_1 PP_6 [0.00261203],\n",
       " ATPP_2 -> PP_3 CONJP_0 [1.88906e-05],\n",
       " ATPP_2 -> IN_3 NP_4 [9.46374e-06],\n",
       " ATPP_2 -> IN_10 NP_5 [4.31718e-06],\n",
       " ATPP_2 -> PP_6 COMMA_0 [0.0243766],\n",
       " ATPP_2 -> ADVP_13 RB_24 [2.13589e-06],\n",
       " ATPP_2 -> RB_8 IN_1 [1.57825e-05],\n",
       " ATPP_2 -> RB_0 PP_8 [7.0794e-06],\n",
       " ATPP_2 -> ATPP_1 ADVP_1 [0.0075298],\n",
       " ATPP_2 -> PP_15 CC_5 [1.80171e-06],\n",
       " ATPP_2 -> PP_9 CC_3 [5.2147e-07],\n",
       " ATPP_2 -> IN_6 NP_4 [1.55887e-06],\n",
       " ATPP_2 -> COMMA_0 PP_6 [2.30279e-07],\n",
       " ATPP_2 -> RB_2 PP_8 [0.00468905],\n",
       " ATPP_2 -> TICK_0 RB_18 [0.00123577],\n",
       " ATPP_2 -> ATPP_7 FRAG_1 [0.00135622],\n",
       " ATPP_2 -> RB_23 IN_14 [7.80215e-06],\n",
       " ATPP_2 -> RB_25 IN_15 [1.43629e-05],\n",
       " ATPP_2 -> ATPP_2 RB_1 [1.22326e-05],\n",
       " ATPP_2 -> IN_0 NP_1 [2.42702e-05],\n",
       " ATPP_2 -> NP_0 IN_0 [1.78955e-06],\n",
       " ATPP_2 -> IN_14 COMMA_0 [0.000887709],\n",
       " ATPP_2 -> PP_14 CC_3 [7.47248e-07],\n",
       " ATPP_2 -> NP_2 IN_1 [2.13623e-06],\n",
       " ATPP_2 -> RB_24 ADVP_1 [5.3134e-06],\n",
       " ATPP_2 -> PP_8 PRN_4 [0.00134493],\n",
       " ATPP_2 -> IN_2 NP_2 [2.17926e-05],\n",
       " ATPP_2 -> ATPP_1 IN_15 [5.73984e-07],\n",
       " ATPP_2 -> PP_3 COLON_2 [0.00506307],\n",
       " ATPP_2 -> RB_1 PP_4 [0.00520201],\n",
       " ATPP_2 -> PP_14 ADVP_0 [0.000131873],\n",
       " ATPP_2 -> ATPP_2 RB_23 [0.000292049],\n",
       " ATPP_2 -> TICK_1 RB_25 [0.00179073],\n",
       " ATPP_2 -> ADVP_0 PP_15 [0.00105203],\n",
       " ATPP_2 -> RB_14 RB_7 [0.00129061],\n",
       " ATPP_2 -> ATPP_7 ADJP_14 [0.00079195],\n",
       " ATPP_2 -> NP_2 IN_0 [1.68218e-06],\n",
       " ATPP_2 -> ADVP_1 RB_2 [8.66985e-06],\n",
       " ATPP_2 -> ATPP_1 PP_8 [0.000767521],\n",
       " ATPP_2 -> PP_12 CC_4 [2.81169e-07],\n",
       " ATPP_2 -> ATPP_1 COMMA_0 [2.81361e-05],\n",
       " ATPP_2 -> PP_15 ADVP_0 [0.000843103],\n",
       " ATPP_2 -> ATPP_1 IN_19 [4.37479e-07],\n",
       " ATPP_2 -> ATPP_7 NP_2 [0.0595576],\n",
       " ATPP_2 -> ATPP_0 DOT_1 [0.00129164],\n",
       " ATPP_2 -> CONJP_0 PP_9 [0.000764334],\n",
       " ATPP_2 -> IN_2 NP_3 [3.3394e-06],\n",
       " ATPP_2 -> ATPP_2 ADVP_6 [0.000954755],\n",
       " ATPP_2 -> RB_16 PP_6 [0.00104256],\n",
       " ATPP_2 -> PP_6 CC_3 [1.14352e-06],\n",
       " ATPP_2 -> PP_8 COLON_4 [0.00248145],\n",
       " ATPP_2 -> PP_8 CONJP_0 [5.40952e-05],\n",
       " ATPP_2 -> PP_0 COLON_2 [0.00164826],\n",
       " ATPP_2 -> CONJP_0 PP_14 [0.000760985],\n",
       " ATPP_2 -> ATPP_1 PP_7 [8.83432e-05],\n",
       " ATPP_2 -> RB_11 PP_14 [0.0127289],\n",
       " ATPP_2 -> ATPP_2 RB_19 [5.26486e-06],\n",
       " ATPP_2 -> PP_8 CC_3 [2.90195e-05],\n",
       " ATPP_2 -> RB_2 PP_5 [0.000214776],\n",
       " ATPP_2 -> ATPP_7 NP_16 [0.0381598],\n",
       " ATPP_2 -> ATPP_2 PP_15 [0.00163103],\n",
       " ATPP_2 -> IN_5 NP_0 [1.52193e-05],\n",
       " ATPP_2 -> IN_0 NP_2 [2.26444e-05],\n",
       " ATPP_2 -> ATPP_1 IN_18 [8.18182e-06],\n",
       " ATPP_2 -> ATPP_1 PP_4 [0.0109538],\n",
       " ATPP_2 -> PP_5 ADVP_1 [1.27203e-05],\n",
       " ATPP_2 -> PP_4 COLON_3 [0.000121348],\n",
       " ATPP_2 -> RB_19 RB_0 [0.00179393],\n",
       " ATPP_2 -> ATPP_2 RB_2 [3.52247e-05],\n",
       " ATPP_2 -> RB_24 RB_4 [0.000222149],\n",
       " ATPP_2 -> IN_3 PRPDOLLAR_3 [0.00181578],\n",
       " ATPP_2 -> ATPP_1 PP_15 [2.10033e-06],\n",
       " ATPP_2 -> ATPP_3 S_4 [1.28966e-05],\n",
       " ATPP_2 -> IN_3 NP_5 [0.000165057],\n",
       " ATPP_2 -> IN_4 NP_4 [1.00001e-05],\n",
       " ATPP_2 -> RB_19 JJ_6 [2.29752e-06],\n",
       " ATPP_2 -> ATPP_2 PP_4 [0.00118054],\n",
       " ATPP_2 -> IN_13 NP_22 [3.14103e-06],\n",
       " ATPP_2 -> RB_23 IN_0 [8.91825e-07],\n",
       " ATPP_2 -> PP_1 COLON_3 [0.00190858],\n",
       " ATPP_2 -> RB_14 PP_7 [0.000843174],\n",
       " ATPP_2 -> CONJP_0 PP_15 [1.11484e-05],\n",
       " ATPP_2 -> ADVP_15 RB_24 [3.52576e-06],\n",
       " ATPP_2 -> RB_22 IN_16 [7.30809e-06],\n",
       " ATPP_2 -> IN_0 NP_26 [9.26257e-07],\n",
       " ATPP_2 -> RB_14 PP_5 [0.00200703],\n",
       " ATPP_2 -> ATPP_2 ADVP_11 [5.32094e-05],\n",
       " ATPP_2 -> ATPP_1 ADVP_12 [2.19263e-06],\n",
       " ATPP_2 -> PP_13 COMMA_0 [1.91452e-06],\n",
       " ATPP_2 -> RB_0 PP_4 [0.0013306],\n",
       " ATPP_2 -> PP_2 COMMA_0 [0.030284],\n",
       " ATPP_2 -> IN_1 NP_5 [7.98637e-05],\n",
       " ATPP_2 -> PP_6 COLON_0 [4.45437e-06],\n",
       " ATPP_2 -> IN_1 COMMA_0 [2.32876e-05],\n",
       " ATPP_2 -> PP_2 COLON_2 [1.76307e-05],\n",
       " ATPP_2 -> IN_10 NP_26 [3.18325e-05],\n",
       " ATPP_2 -> ATPP_7 NP_18 [0.0091473],\n",
       " ATPP_2 -> RB_19 PP_6 [0.000456221],\n",
       " ATPP_2 -> PP_9 COMMA_0 [1.40234e-07],\n",
       " ATPP_2 -> ATPP_1 PP_3 [7.83609e-05],\n",
       " ATPP_2 -> RB_4 PP_8 [0.00884583],\n",
       " ATPP_2 -> IN_5 NP_1 [9.45283e-06],\n",
       " ATPP_2 -> ATPP_2 CC_5 [0.000851604],\n",
       " ATPP_2 -> IN_0 NP_6 [6.13026e-05],\n",
       " ATPP_2 -> IN_10 NP_24 [0.000132447],\n",
       " ATPP_2 -> ATPP_1 PP_1 [0.000233834],\n",
       " ATPP_2 -> RB_14 PP_0 [0.00220189],\n",
       " ATPP_2 -> RB_14 PP_8 [0.00407793],\n",
       " ATPP_2 -> IN_2 COMMA_0 [0.00265089],\n",
       " ATPP_2 -> PP_6 COLON_4 [0.000302207],\n",
       " ATPP_2 -> IN_4 NP_1 [4.30064e-06],\n",
       " ATPP_2 -> PP_8 COMMA_0 [0.149139],\n",
       " ATPP_2 -> CONJP_0 PP_5 [0.00385912],\n",
       " ATPP_2 -> RB_24 PP_7 [0.000965101],\n",
       " ATPP_2 -> ATPP_1 PP_5 [7.51283e-06],\n",
       " ATPP_2 -> IN_11 NP_21 [1.65942e-06],\n",
       " ATPP_2 -> ADVP_11 RB_25 [4.37792e-06],\n",
       " ATPP_2 -> PP_0 CC_4 [9.42731e-06],\n",
       " ATPP_2 -> RB_11 IN_1 [1.00748e-06],\n",
       " ATPP_2 -> PP_8 RB_2 [3.27716e-06],\n",
       " ATPP_2 -> PP_7 COLON_0 [0.000288979],\n",
       " ATPP_2 -> PP_4 CONJP_0 [2.06851e-05],\n",
       " ATPP_2 -> COMMA_0 PP_7 [0.000466054],\n",
       " ATPP_2 -> PP_1 CC_3 [1.67098e-07],\n",
       " ATPP_2 -> RB_11 IN_0 [4.76618e-06],\n",
       " ATPP_2 -> RB_2 PP_6 [0.00278787],\n",
       " ATPP_2 -> RB_23 ADVP_0 [3.65336e-06],\n",
       " ATPP_2 -> ATPP_2 CONJP_0 [9.79718e-05],\n",
       " ATPP_2 -> PP_6 CC_5 [5.25279e-05],\n",
       " ATPP_2 -> RB_7 PP_15 [0.000269406],\n",
       " ATPP_2 -> PP_15 COLON_1 [0.000522658],\n",
       " ATPP_2 -> IN_5 NP_26 [8.03531e-05],\n",
       " ATPP_2 -> PP_5 CC_4 [7.30628e-07],\n",
       " ATPP_2 -> RB_22 PP_0 [0.00184221],\n",
       " ATPP_2 -> CC_0 PP_14 [2.59354e-06],\n",
       " ATPP_2 -> PP_7 CC_4 [2.91697e-05],\n",
       " ATPP_2 -> ATPP_7 NP_17 [0.00128761],\n",
       " ATPP_2 -> PP_7 RB_7 [2.33115e-06],\n",
       " ATPP_2 -> COMMA_0 PP_8 [0.000437819],\n",
       " ATPP_2 -> ATPP_7 IN_15 [4.81757e-07],\n",
       " ATPP_2 -> ATPP_1 ADVP_0 [3.43001e-05],\n",
       " ATPP_2 -> ATPP_2 ADVP_14 [0.00485876],\n",
       " ATPP_2 -> IN_1 NP_1 [0.000522212],\n",
       " ATPP_2 -> ATPP_2 RB_8 [6.3157e-06],\n",
       " ATPP_2 -> ATPP_2 ADVP_0 [1.82488e-05],\n",
       " ATPP_2 -> ADVP_16 RB_24 [6.6298e-06],\n",
       " ATPP_2 -> IN_2 NP_5 [0.000299866],\n",
       " ATPP_2 -> IN_4 NP_26 [4.68546e-05],\n",
       " ATPP_2 -> IN_5 NP_14 [5.52873e-07],\n",
       " ATPP_2 -> ADVP_1 RB_23 [0.00217001],\n",
       " ATPP_2 -> RB_2 PP_4 [2.5013e-05],\n",
       " ATPP_2 -> IN_13 NP_23 [1.28734e-05],\n",
       " ATPP_2 -> PP_15 COMMA_0 [0.00739896],\n",
       " ATPP_2 -> PP_1 COMMA_0 [0.0431836],\n",
       " ATPP_2 -> PP_7 PP_15 [2.2432e-06],\n",
       " ATPP_2 -> TICK_0 RB_25 [0.00107039],\n",
       " ATPP_2 -> IN_2 NP_26 [3.10956e-05],\n",
       " ATPP_2 -> PP_8 COLON_2 [0.000534587],\n",
       " ATPP_2 -> RB_25 ADVP_0 [7.04771e-07],\n",
       " ATPP_2 -> IN_5 NP_4 [9.36451e-07],\n",
       " ATPP_2 -> ATPP_7 IN_10 [6.77521e-06],\n",
       " ATPP_2 -> PP_4 CC_3 [3.4471e-06],\n",
       " ATPP_2 -> IN_0 COMMA_0 [0.000955365],\n",
       " ATPP_2 -> ATPP_5 S_4 [1.2224e-06],\n",
       " ATPP_2 -> CONJP_0 PP_8 [4.85386e-07],\n",
       " ATPP_2 -> PP_4 CC_5 [7.45055e-05],\n",
       " ATPP_2 -> ATPP_2 ADVP_1 [0.00121151],\n",
       " ATPP_2 -> ATPP_7 NP_6 [3.5534e-05],\n",
       " ATPP_2 -> ATPP_2 CC_3 [9.66817e-05],\n",
       " ATPP_2 -> PP_3 COMMA_0 [0.0219568],\n",
       " ATPP_2 -> ATPP_1 RB_2 [4.731e-05],\n",
       " ATPP_2 -> PP_5 COLON_3 [0.000933993],\n",
       " ATPP_2 -> RB_14 PP_15 [0.0003683],\n",
       " ATPP_2 -> PP_7 CONJP_0 [2.50398e-05],\n",
       " ATPP_2 -> IN_10 NP_1 [3.47788e-06],\n",
       " ATPP_2 -> ATPP_7 SBARQ_0 [0.00180829],\n",
       " ATPP_2 -> PP_2 CC_4 [0.000449983],\n",
       " ATPP_2 -> RB_22 PP_8 [0.00256939],\n",
       " ATPP_2 -> PP_3 CC_5 [2.37001e-07],\n",
       " ATPP_2 -> ADVP_14 RB_23 [1.58881e-05],\n",
       " ATPP_2 -> IN_3 NP_14 [0.000122178],\n",
       " ATPP_2 -> IN_1 NP_0 [2.16813e-06],\n",
       " ATPP_2 -> ATPP_7 S_5 [0.00544447],\n",
       " ATPP_2 -> ATPP_1 RB_19 [8.81171e-06],\n",
       " ATPP_2 -> RB_1 PP_5 [0.000129967],\n",
       " ATPP_2 -> PP_3 COLON_1 [0.00456257],\n",
       " ATPP_2 -> IN_1 NP_25 [3.95024e-06],\n",
       " ATPP_2 -> ADVP_6 RB_11 [1.71758e-05],\n",
       " ATPP_2 -> CC_0 PP_15 [0.00723057],\n",
       " ATPP_2 -> IN_1 NP_6 [2.89132e-05],\n",
       " ATPP_2 -> RB_0 ADVP_1 [1.23508e-06],\n",
       " ATPP_2 -> PP_5 CC_5 [3.27493e-05],\n",
       " ATPP_2 -> IN_3 NP_26 [0.0002461],\n",
       " ATPP_2 -> ATPP_1 RB_22 [2.1746e-05],\n",
       " ATPP_2 -> PP_14 COMMA_0 [0.0122256],\n",
       " ATPP_2 -> IN_3 NP_2 [6.82083e-05],\n",
       " ATPP_2 -> ATPP_1 RB_8 [0.000112863],\n",
       " ATPP_2 -> ATPP_7 IN_16 [1.95997e-05],\n",
       " ATPP_2 -> ATPP_2 RB_11 [0.000234355],\n",
       " ATPP_2 -> ATPP_1 PP_14 [0.00176344],\n",
       " ATPP_2 -> PP_4 CC_4 [0.000361486],\n",
       " ATPP_2 -> IN_10 NP_0 [4.33174e-05],\n",
       " ATPP_2 -> IN_2 NP_4 [0.000183785],\n",
       " ATPP_2 -> ATPP_2 COLON_1 [0.000904146],\n",
       " ATPP_2 -> ATPP_7 NP_4 [0.00068002],\n",
       " ATPP_2 -> IN_1 NP_2 [3.30502e-05],\n",
       " ATPP_2 -> PP_4 COMMA_0 [0.0485727],\n",
       " ATPP_2 -> ATPP_2 PP_8 [0.0272258],\n",
       " ATPP_2 -> CONJP_0 PP_7 [0.00263025],\n",
       " ATPP_2 -> PP_6 CONJP_0 [8.49633e-06],\n",
       " ATPP_2 -> RB_16 PP_5 [0.00108352],\n",
       " ATPP_2 -> ATPP_2 ADVP_13 [1.7148e-07],\n",
       " ATPP_2 -> RB_7 RB_19 [0.000452819],\n",
       " ATPP_2 -> RB_25 PP_4 [0.00046835],\n",
       " ATPP_2 -> IN_0 NP_5 [6.83165e-05],\n",
       " ATPP_2 -> ATPP_7 ADJP_15 [0.005085],\n",
       " ATPP_2 -> IN_2 S_5 [1.74572e-05],\n",
       " ATPP_2 -> PP_0 CC_5 [4.22088e-05],\n",
       " ATPP_2 -> ATPP_0 COMMA_0 [0.00100493],\n",
       " ATPP_2 -> PP_8 CC_5 [0.00236835],\n",
       " ATPP_2 -> RB_11 PP_15 [9.40862e-06],\n",
       " ATPP_2 -> VB_15 CC_4 [0.00180829],\n",
       " ATPP_2 -> ADVP_0 PP_14 [0.00527661],\n",
       " ATPP_2 -> ATPP_2 PP_1 [0.0114736],\n",
       " ATPP_2 -> ADVP_6 RB_22 [1.56556e-05],\n",
       " ATPP_2 -> IN_5 NP_3 [7.96999e-05],\n",
       " ATPP_2 -> IN_2 NP_25 [1.6421e-05],\n",
       " ATPP_2 -> ATPP_7 IN_14 [2.8906e-05],\n",
       " ATPP_2 -> IN_10 NP_2 [2.1653e-05],\n",
       " ATPP_2 -> DT_11 PP_8 [0.0105414],\n",
       " ATPP_2 -> ATPP_1 ADVP_11 [5.15819e-05],\n",
       " ATPP_2 -> RB_14 PP_4 [0.000395974],\n",
       " ATPP_2 -> PP_14 CONJP_0 [4.48984e-06],\n",
       " ATPP_2 -> IN_10 NP_25 [1.47534e-05],\n",
       " ATPP_2 -> RB_14 PP_14 [0.00350822],\n",
       " ATPP_2 -> ATPP_1 RB_23 [0.000144799],\n",
       " ATPP_2 -> IN_1 TICK_0 [6.50327e-06],\n",
       " ATPP_2 -> IN_4 NP_24 [2.49768e-05],\n",
       " ATPP_2 -> DT_8 PP_8 [0.0071195],\n",
       " ATPP_2 -> ATPP_2 CC_4 [0.000223187],\n",
       " ATPP_2 -> IN_10 PRPDOLLAR_3 [0.000434928],\n",
       " ATPP_2 -> RB_19 PP_4 [0.000292394],\n",
       " ATPP_2 -> TO_0 NP_17 [8.34239e-06],\n",
       " ATPP_2 -> IN_4 NP_0 [3.55454e-06],\n",
       " ATPP_2 -> PP_8 COLON_1 [5.22245e-05],\n",
       " ATPP_2 -> IN_4 NP_2 [6.18303e-06],\n",
       " ATPP_2 -> PP_15 CC_4 [5.21978e-06],\n",
       " ATPP_2 -> CONJP_0 PP_6 [0.00108676],\n",
       " ATPP_2 -> PP_6 CC_4 [5.01971e-06],\n",
       " ATPP_2 -> ATPP_1 PRN_3 [6.8461e-06],\n",
       " ATPP_2 -> PP_8 COLON_0 [0.000745878],\n",
       " ATPP_2 -> CONJP_0 PP_4 [0.00943635],\n",
       " ATPP_2 -> ATPP_7 NP_3 [0.0170401],\n",
       " ATPP_2 -> RB_8 JJ_21 [2.30775e-06],\n",
       " ATPP_2 -> ATPP_7 IN_0 [5.45406e-07],\n",
       " ATPP_2 -> PP_10 CC_5 [1.53259e-05],\n",
       " ATPP_2 -> ATPP_2 DOT_1 [2.89958e-06],\n",
       " ATPP_2 -> ATPP_2 PP_0 [0.00194958],\n",
       " ATPP_2 -> IN_1 NP_26 [2.30399e-05],\n",
       " ATPP_2 -> TICK_0 RB_22 [0.000823216],\n",
       " ATPP_2 -> ATPP_2 RB_22 [1.71227e-05],\n",
       " ATPP_2 -> PP_3 CC_4 [4.53656e-06],\n",
       " ATPP_2 -> ATPP_1 RB_11 [0.000129403],\n",
       " ATPP_2 -> IN_0 NP_25 [1.41088e-06],\n",
       " ATPP_2 -> TICK_1 ADVP_0 [0.000449117],\n",
       " ATPP_2 -> PP_2 CC_3 [0.000104601],\n",
       " ATPP_2 -> IN_4 NP_25 [5.08954e-05],\n",
       " ATPP_2 -> PP_1 CC_5 [7.85028e-05],\n",
       " ATPP_2 -> PP_7 ADVP_0 [0.00290168],\n",
       " ATPP_2 -> PP_4 ADVP_1 [0.00273623],\n",
       " ATPP_2 -> PP_8 ADVP_1 [0.00151053],\n",
       " ATPP_2 -> PP_5 CC_3 [5.33756e-07],\n",
       " ATPP_2 -> IN_2 NP_14 [7.64588e-05],\n",
       " ATPP_2 -> PP_14 CC_5 [0.000204125],\n",
       " ATPP_2 -> RB_2 PP_7 [0.00257342],\n",
       " ATPP_2 -> PP_7 COMMA_0 [0.062062],\n",
       " ATPP_2 -> ATPP_7 NP_5 [3.74009e-05],\n",
       " ATPP_2 -> PP_2 CC_5 [0.000306131],\n",
       " ATPP_2 -> ATPP_2 PP_14 [0.00116142],\n",
       " ATPP_2 -> ATPP_7 S_6 [0.000918487],\n",
       " ATPP_2 -> ADVP_17 RB_25 [3.61015e-07],\n",
       " ATPP_2 -> RB_14 PP_1 [0.00973403],\n",
       " ATPP_2 -> ATPP_7 IN_1 [3.12427e-05],\n",
       " ATPP_2 -> VBG_20 PP_7 [0.0013556],\n",
       " ATPP_2 -> IN_0 NP_3 [0.000823448],\n",
       " ATPP_2 -> PP_7 PRN_2 [0.000915435],\n",
       " ATPP_2 -> ATPP_2 ADVP_9 [0.00407264],\n",
       " ATPP_2 -> PP_5 CONJP_0 [5.09723e-06],\n",
       " ATPP_2 -> ADVP_15 RB_25 [2.17017e-06],\n",
       " ATPP_2 -> IN_5 NP_25 [0.000243795],\n",
       " ATPP_2 -> RB_14 PP_6 [0.00231667],\n",
       " ATPP_2 -> IN_0 NP_4 [3.16322e-06],\n",
       " ATPP_2 -> IN_3 NP_25 [1.81595e-05],\n",
       " ATPP_2 -> IN_5 NP_5 [0.000120789],\n",
       " ATPP_2 -> RB_19 IN_10 [8.69741e-06],\n",
       " ATPP_2 -> PP_15 COLON_3 [0.000993675],\n",
       " ATPP_2 -> RB_8 PP_7 [0.00337845],\n",
       " ATPP_2 -> ATPP_2 PP_5 [0.000628224],\n",
       " ATPP_2 -> IN_2 PRPDOLLAR_3 [9.6522e-06],\n",
       " ATPP_2 -> RB_11 IN_10 [3.18799e-06],\n",
       " ATPP_2 -> PP_7 CC_3 [1.68178e-05],\n",
       " ATPP_2 -> IN_10 NP_3 [0.000139649],\n",
       " ATPP_2 -> PP_1 CONJP_0 [1.87523e-06],\n",
       " ATPP_2 -> DT_8 PP_6 [0.00313322],\n",
       " ATPP_2 -> PP_0 COLON_0 [0.00190814],\n",
       " ATPP_2 -> PP_9 CC_5 [3.03601e-05],\n",
       " ATPP_2 -> ATPP_1 PP_2 [0.00374073],\n",
       " ATPP_2 -> RB_23 ADVP_1 [3.83334e-06],\n",
       " ATPP_2 -> ATPP_0 DOT_2 [0.00458519],\n",
       " ATPP_2 -> PP_7 COLON_3 [1.53777e-06],\n",
       " ATPP_2 -> RB_24 PP_8 [0.00137781],\n",
       " ATPP_2 -> IN_3 NP_3 [1.14494e-07],\n",
       " ATPP_2 -> PP_0 CC_3 [4.22435e-07],\n",
       " ATPP_2 -> PP_7 COLON_1 [0.000561984],\n",
       " ATPP_2 -> IN_4 NP_5 [1.12315e-05],\n",
       " ATPP_2 -> IN_11 NP_20 [3.43384e-06],\n",
       " ATPP_2 -> PP_1 CC_4 [4.97828e-06],\n",
       " ATPP_2 -> ATPP_1 ADVP_16 [1.01651e-05],\n",
       " ATPP_2 -> IN_1 NP_4 [1.28386e-05],\n",
       " ATPP_2 -> IN_0 NP_0 [1.78322e-05],\n",
       " ATPP_2 -> PP_5 COMMA_0 [0.0265245],\n",
       " ATPP_2 -> PP_0 COMMA_0 [0.017469],\n",
       " ATPP_2 -> ATPP_2 COMMA_0 [0.112027],\n",
       " ATPP_2 -> IN_1 NP_3 [0.000690123],\n",
       " ATPP_2 -> PP_1 COLON_0 [0.000190068],\n",
       " ATPP_2 -> PP_10 COMMA_0 [7.37574e-05],\n",
       " ATPP_2 -> ATPP_1 ADVP_6 [7.59815e-05],\n",
       " ATPP_2 -> PP_7 CC_5 [0.00021532],\n",
       " ATPP_2 -> RB_14 RB_11 [0.000984018],\n",
       " ATPP_2 -> RB_7 RB_11 [0.00104794],\n",
       " ATPP_2 -> ATPP_7 JJ_21 [0.000904127],\n",
       " ATPP_2 -> PP_8 CC_4 [5.56638e-05],\n",
       " ATPP_2 -> ATPP_2 PP_6 [0.00252957],\n",
       " ATPP_2 -> ATPP_2 CC_0 [0.00163861],\n",
       " ATPP_2 -> ATPP_2 PP_7 [0.00592109]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonterminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Creating subset PCFG with top k = 0.2... ************\n",
      "Starting with 2513065 productions.\n",
      "Created subset PCFG with a length of 2228320 productions.\n",
      "Cleaning subset: (1) removing unreachable productions...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.2\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     PRODS_SEEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()  \u001b[38;5;66;03m# to prevent recursion\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     subset_pcfg, subset_pcfg_pos \u001b[38;5;241m=\u001b[39m \u001b[43mload_subset_pcfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprod_productions_v2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlexical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 66\u001b[0m, in \u001b[0;36mload_subset_pcfg\u001b[0;34m(prob_productions, top_k, save, load, lexical)\u001b[0m\n\u001b[1;32m     62\u001b[0m             subset_pcfg_pos \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m subset_pcfg, subset_pcfg_pos\n\u001b[0;32m---> 66\u001b[0m subset_pcfg, subset_pcfg_pos \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_subset_pcfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob_productions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlexical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlexical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subset_pcfg, subset_pcfg_pos\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mcreate_subset_pcfg\u001b[0;34m(productions, top_k, no_recursion, save, lexical)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreated subset PCFG with a length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(subset_productions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m productions.\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaning subset: (1) removing unreachable productions...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m final_subset_productions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreachable_productions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset_productions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_recursion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_recursion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# update set for removed recursive productions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m reachable_nonterminals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(prod\u001b[38;5;241m.\u001b[39mlhs() \u001b[38;5;28;01mfor\u001b[39;00m prod \u001b[38;5;129;01min\u001b[39;00m final_subset_productions)\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mreachable_productions\u001b[0;34m(productions, lhs, parents, no_recursion)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rhs \u001b[38;5;129;01min\u001b[39;00m prod\u001b[38;5;241m.\u001b[39mrhs():\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rhs, Nonterminal):\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m reachable_productions(\n\u001b[1;32m     25\u001b[0m             productions, \n\u001b[1;32m     26\u001b[0m             rhs, \n\u001b[1;32m     27\u001b[0m             parents\u001b[38;5;241m=\u001b[39mnew_parents,\n\u001b[1;32m     28\u001b[0m             no_recursion\u001b[38;5;241m=\u001b[39mno_recursion,\n\u001b[1;32m     29\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mreachable_productions\u001b[0;34m(productions, lhs, parents, no_recursion)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rhs \u001b[38;5;129;01min\u001b[39;00m prod\u001b[38;5;241m.\u001b[39mrhs():\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rhs, Nonterminal):\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m reachable_productions(\n\u001b[1;32m     25\u001b[0m             productions, \n\u001b[1;32m     26\u001b[0m             rhs, \n\u001b[1;32m     27\u001b[0m             parents\u001b[38;5;241m=\u001b[39mnew_parents,\n\u001b[1;32m     28\u001b[0m             no_recursion\u001b[38;5;241m=\u001b[39mno_recursion,\n\u001b[1;32m     29\u001b[0m         )\n",
      "    \u001b[0;31m[... skipping similar frames: reachable_productions at line 24 (4 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mreachable_productions\u001b[0;34m(productions, lhs, parents, no_recursion)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rhs \u001b[38;5;129;01min\u001b[39;00m prod\u001b[38;5;241m.\u001b[39mrhs():\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rhs, Nonterminal):\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m reachable_productions(\n\u001b[1;32m     25\u001b[0m             productions, \n\u001b[1;32m     26\u001b[0m             rhs, \n\u001b[1;32m     27\u001b[0m             parents\u001b[38;5;241m=\u001b[39mnew_parents,\n\u001b[1;32m     28\u001b[0m             no_recursion\u001b[38;5;241m=\u001b[39mno_recursion,\n\u001b[1;32m     29\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mreachable_productions\u001b[0;34m(productions, lhs, parents, no_recursion)\u001b[0m\n\u001b[1;32m      6\u001b[0m new_parents \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39mparents, lhs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# select productions belonging to the current lhs\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m lhs_productions \u001b[38;5;241m=\u001b[39m [prod \u001b[38;5;28;01mfor\u001b[39;00m prod \u001b[38;5;129;01min\u001b[39;00m productions \u001b[38;5;28;01mif\u001b[39;00m prod\u001b[38;5;241m.\u001b[39mlhs() \u001b[38;5;241m==\u001b[39m lhs]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prod \u001b[38;5;129;01min\u001b[39;00m lhs_productions:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (prod,) \u001b[38;5;129;01min\u001b[39;00m PRODS_SEEN:\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m new_parents \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39mparents, lhs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# select productions belonging to the current lhs\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m lhs_productions \u001b[38;5;241m=\u001b[39m [prod \u001b[38;5;28;01mfor\u001b[39;00m prod \u001b[38;5;129;01min\u001b[39;00m productions \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mprod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlhs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlhs\u001b[49m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prod \u001b[38;5;129;01min\u001b[39;00m lhs_productions:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (prod,) \u001b[38;5;129;01min\u001b[39;00m PRODS_SEEN:\n",
      "File \u001b[0;32m~/Documents/Master/Jaar_3/Thesis/thesis_code/.venv/lib/python3.9/site-packages/nltk/grammar.py:123\u001b[0m, in \u001b[0;36mNonterminal.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Return the node value corresponding to this ``Nonterminal``.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    :rtype: (any)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    Return True if this non-terminal is equal to ``other``.  In\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m    particular, return True if ``other`` is a ``Nonterminal``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    :rtype: bool\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39m_symbol\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in [0.2]:\n",
    "    PRODS_SEEN = set()  # to prevent recursion\n",
    "    subset_pcfg, subset_pcfg_pos = load_subset_pcfg(prod_productions_v2, top_k=k, save=False, load=False, lexical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 0.8\n",
    "with open(f'grammars/nltk/subset_pcfg_{top_k}.pkl', 'rb') as f:\n",
    "    subset_pcfg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found her from his head and turned up from his Something . \n",
      "\n",
      "There are going to end the rest . \n",
      "\n",
      "Lisa ran a chance . \n",
      "\n",
      "He <apostrophe>d survived in the pain before she were a Liquid prize . \n",
      "\n",
      "I face no food he are concerned and there tried to do . \n",
      "\n",
      "Did , a boy had left a second his will to leave with him , but I ca n<apostrophe>t need you . \n",
      "\n",
      "You was someone she can Guess n<apostrophe>t be killed . \n",
      "\n",
      "`` I have me open , <apostrophe><apostrophe> she turns . \n",
      "\n",
      "`` Are them , <apostrophe> he dropped his hands with my upper direction , sitting something and bodies . \n",
      "\n",
      "`` She was a giant chance . \n",
      "\n",
      "Edwin saw Kaori on the scene , and it would simply be up as I can take him out . \n",
      "\n",
      "Everything , he will be free to feel a pair of girls if it said she was almost growing very old . <apostrophe><apostrophe> \n",
      "\n",
      "Kevin jumped back with a try that took around . \n",
      "\n",
      "Pierre made her voice , glancing up like a good man for the bedroom . \n",
      "\n",
      "`` I would be the little citizen of one nonstop way , <apostrophe><apostrophe> I said , `` There will be so skilled . \n",
      "\n",
      "It continued and placed them in agonizing danger . \n",
      "\n",
      "`` Simon , <apostrophe><apostrophe> you struggled the other as they had transported to his second Glen Banker <apostrophe>s side before screaming as you cared on a letter <apostrophe>s hand that she are . \n",
      "\n",
      "He had simply killed her to question . \n",
      "\n",
      "Then you asked When he went to have the sun . \n",
      "\n",
      "` tell stale fire error the door she will ensure they had let I felt she <apostrophe>d sent his nothing , My vehicle , <apostrophe><apostrophe> Lucas came , doing her feet directed in her on the path . \n",
      "\n",
      "He figured Calvin needed to see humans of pain . \n",
      "\n",
      "The folks of energy got with flames . \n",
      "\n",
      "In their new laughter , he are the door wandering gray holes . \n",
      "\n",
      "The noise throbbed in animals . \n",
      "\n",
      "Francisco had stopped . \n",
      "\n",
      "moving in her son on something when he took the darkness around me as I saw him quickly at the motions , Finan felt waves to need a chuckle for her family . \n",
      "\n",
      "His entire faith appeared chatting with his hosts and saw up to changes of mine . \n",
      "\n",
      "found the past lights smiling into the right thought of guilt , Mike left her for a few slaves , the door replies . \n",
      "\n",
      "He has coming and instead . <apostrophe><apostrophe> \n",
      "\n",
      "He <apostrophe>re a ogre tonight on that ship and a black business was on her forehead . \n",
      "\n",
      "`` let Paul tell if it can worry , before opening it ? <apostrophe><apostrophe> \n",
      "\n",
      "Eda dropped suddenly into his head . \n",
      "\n",
      "For the whole states of the grass , he prayed the enemy come in my whole , compassionate societies . \n",
      "\n",
      "`` It were the time in exchange . <apostrophe><apostrophe> \n",
      "\n",
      "`` He <apostrophe>re living , to get it up from his tail , <apostrophe><apostrophe> it thought he saw the front sergeant coming in the gun <apostrophe>s search . \n",
      "\n",
      "`` He are lots of the comforting army , <apostrophe> it said until they took tomorrow from the lid . \n",
      "\n",
      "`` Not as I was n<apostrophe>t enjoying chains on that one , there stopped nothin by tomorrow . \n",
      "\n",
      "If he could learn to fall , he must have decked up . <apostrophe><apostrophe> \n",
      "\n",
      "`` So Finally bound , there know that I will mind a lingering time to await the group of the capital ? <apostrophe><apostrophe> \n",
      "\n",
      "The knife made you say . \n",
      "\n",
      "I had to fly back . \n",
      "\n",
      "when she splintered , she replied . \n",
      "\n",
      "Nothing would be willing to take the owner at his feet , and I was a bag , `` Completely . \n",
      "\n",
      "That would n<apostrophe>t make blackness . \n",
      "\n",
      "`` And first , for the actual stick bag , he slipped at her head , down . \n",
      "\n",
      "no kind as they finally rang to the current porch , while they was away , I would be next ? <apostrophe><apostrophe> \n",
      "\n",
      "The man are very aged , and dirty . \n",
      "\n",
      "Sampson exits his dress and was probably waved his back . \n",
      "\n",
      "He still could be sure with this time . \n",
      "\n",
      "The victim was warm . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    for string in generate_pcfg(subset_pcfg, depth=100):\n",
    "        print(\" \".join(string), \"\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to convert pcfg.pkl to pcfg.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_k in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    with open(f'grammars/nltk/pkl/subset_pcfg_{top_k}_pos.pkl', 'rb') as f:\n",
    "        subset_pcfg = pickle.load(f)\n",
    "\n",
    "    f = open(f'grammars/nltk/pos/subset_pcfg_{top_k}.txt', 'w')\n",
    "    for prod in subset_pcfg.productions():\n",
    "        lhs = prod.lhs()\n",
    "        # Check each element in rhs; if it's a string, add quotations\n",
    "        rhs_with_quotes = []\n",
    "        for item in prod.rhs():\n",
    "            if isinstance(item, str):  # Assuming terminals are represented as strings\n",
    "                rhs_with_quotes.append(f\"\\'{item}\\'\")\n",
    "            else:\n",
    "                rhs_with_quotes.append(str(item))\n",
    "        rhs_formatted = ' '.join(rhs_with_quotes)\n",
    "        prob = prod.prob()\n",
    "        # Format the probability as a decimal float with desired prec\n",
    "        # ision, e.g., 10 decimal places\n",
    "        formatted_prob = f'{prob:.10f}'\n",
    "        # Recreate the production string with the formatted probability and rhs with quotations\n",
    "        prod_str = f\"{lhs} -> {rhs_formatted} [{formatted_prob}]\"\n",
    "        f.write(f\"{prod_str}\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
